{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install featuretools","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: featuretools in /opt/conda/lib/python3.7/site-packages (0.17.0)\nRequirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from featuretools) (1.4.1)\nRequirement already satisfied: dask[dataframe]>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from featuretools) (2.20.0)\nRequirement already satisfied: pyyaml>=3.12 in /opt/conda/lib/python3.7/site-packages (from featuretools) (5.3.1)\nRequirement already satisfied: pandas>=0.24.1 in /opt/conda/lib/python3.7/site-packages (from featuretools) (1.0.3)\nRequirement already satisfied: psutil>=5.4.8 in /opt/conda/lib/python3.7/site-packages (from featuretools) (5.7.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from featuretools) (1.18.5)\nRequirement already satisfied: cloudpickle>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from featuretools) (1.3.0)\nRequirement already satisfied: click>=7.0.0 in /opt/conda/lib/python3.7/site-packages (from featuretools) (7.1.1)\nRequirement already satisfied: tqdm>=4.32.0 in /opt/conda/lib/python3.7/site-packages (from featuretools) (4.45.0)\nRequirement already satisfied: distributed>=1.24.2 in /opt/conda/lib/python3.7/site-packages (from featuretools) (2.14.0)\nRequirement already satisfied: toolz>=0.8.2; extra == \"dataframe\" in /opt/conda/lib/python3.7/site-packages (from dask[dataframe]>=1.1.0->featuretools) (0.10.0)\nRequirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in /opt/conda/lib/python3.7/site-packages (from dask[dataframe]>=1.1.0->featuretools) (0.7.2)\nRequirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in /opt/conda/lib/python3.7/site-packages (from dask[dataframe]>=1.1.0->featuretools) (1.1.0)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.1->featuretools) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.1->featuretools) (2019.3)\nRequirement already satisfied: zict>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (2.0.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (46.1.3.post20200325)\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (2.1.0)\nRequirement already satisfied: tblib>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (1.6.0)\nRequirement already satisfied: msgpack>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (1.0.0)\nRequirement already satisfied: tornado>=5; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (5.0.2)\nRequirement already satisfied: locket in /opt/conda/lib/python3.7/site-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[dataframe]>=1.1.0->featuretools) (0.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.24.1->featuretools) (1.14.0)\nRequirement already satisfied: heapdict in /opt/conda/lib/python3.7/site-packages (from zict>=0.1.3->distributed>=1.24.2->featuretools) (1.0.1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# featuretools for automated feature engineering\nimport featuretools as ft\n\n# matplotlit and seaborn for visualizations\nimport matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 22\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the datasets and limit to the first 1000 rows (sorted by SK_ID_CURR) \n# This allows us to actually see the results in a reasonable amount of time! \napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv').sort_values('SK_ID_CURR').reset_index(drop = True).loc[:1000, :]\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv').sort_values('SK_ID_CURR').reset_index(drop = True).loc[:1000, :]\nbureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv').sort_values(['SK_ID_CURR', 'SK_ID_BUREAU']).reset_index(drop = True).loc[:1000, :]\nbureau_balance = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv').sort_values('SK_ID_BUREAU').reset_index(drop = True).loc[:1000, :]\ncash = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]\ncredit = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]\nprevious = pd.read_csv('../input/home-credit-default-risk/previous_application.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]\ninstallments = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll join the train and test set together but add a separate column identifying the set. This is important because we are going to want to apply the same exact procedures to each dataset. It's safest to just join them together and treat them as a single dataframe.\n\n(I'm not sure if this is allowing data leakage into the train set and if these feature creation operations should be applied separately. Any thoughts would be much appreciated!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add identifying column\napp_train['set'] = 'train'\napp_test['set'] = 'test'\napp_test[\"TARGET\"] = np.nan\n\n# Append the dataframes\napp = app_train.append(app_test, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Entity set with id applications\nes = ft.EntitySet(id = 'clients')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}