{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":6,"outputs":[{"output_type":"stream","text":"/kaggle/input/home-credit-default-risk/bureau_balance.csv\n/kaggle/input/home-credit-default-risk/credit_card_balance.csv\n/kaggle/input/home-credit-default-risk/previous_application.csv\n/kaggle/input/home-credit-default-risk/application_test.csv\n/kaggle/input/home-credit-default-risk/installments_payments.csv\n/kaggle/input/home-credit-default-risk/bureau.csv\n/kaggle/input/home-credit-default-risk/HomeCredit_columns_description.csv\n/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv\n/kaggle/input/home-credit-default-risk/sample_submission.csv\n/kaggle/input/home-credit-default-risk/application_train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install featuretools","execution_count":7,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: featuretools in /opt/conda/lib/python3.7/site-packages (0.17.0)\nRequirement already satisfied: pyyaml>=3.12 in /opt/conda/lib/python3.7/site-packages (from featuretools) (5.3.1)\nRequirement already satisfied: dask[dataframe]>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from featuretools) (2.20.0)\nRequirement already satisfied: psutil>=5.4.8 in /opt/conda/lib/python3.7/site-packages (from featuretools) (5.7.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from featuretools) (1.18.5)\nRequirement already satisfied: click>=7.0.0 in /opt/conda/lib/python3.7/site-packages (from featuretools) (7.1.1)\nRequirement already satisfied: tqdm>=4.32.0 in /opt/conda/lib/python3.7/site-packages (from featuretools) (4.45.0)\nRequirement already satisfied: distributed>=1.24.2 in /opt/conda/lib/python3.7/site-packages (from featuretools) (2.14.0)\nRequirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from featuretools) (1.4.1)\nRequirement already satisfied: cloudpickle>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from featuretools) (1.3.0)\nRequirement already satisfied: pandas>=0.24.1 in /opt/conda/lib/python3.7/site-packages (from featuretools) (1.0.3)\nRequirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in /opt/conda/lib/python3.7/site-packages (from dask[dataframe]>=1.1.0->featuretools) (1.1.0)\nRequirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in /opt/conda/lib/python3.7/site-packages (from dask[dataframe]>=1.1.0->featuretools) (0.7.2)\nRequirement already satisfied: toolz>=0.8.2; extra == \"dataframe\" in /opt/conda/lib/python3.7/site-packages (from dask[dataframe]>=1.1.0->featuretools) (0.10.0)\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (2.1.0)\nRequirement already satisfied: tblib>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (1.6.0)\nRequirement already satisfied: msgpack>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (1.0.0)\nRequirement already satisfied: zict>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (2.0.0)\nRequirement already satisfied: tornado>=5; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (5.0.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from distributed>=1.24.2->featuretools) (46.1.3.post20200325)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.1->featuretools) (2019.3)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.1->featuretools) (2.8.1)\nRequirement already satisfied: locket in /opt/conda/lib/python3.7/site-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[dataframe]>=1.1.0->featuretools) (0.2.0)\nRequirement already satisfied: heapdict in /opt/conda/lib/python3.7/site-packages (from zict>=0.1.3->distributed>=1.24.2->featuretools) (1.0.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.24.1->featuretools) (1.14.0)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# featuretools for automated feature engineering\nimport featuretools as ft\n\n# matplotlit and seaborn for visualizations\nimport matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 22\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the datasets and limit to the first 1000 rows (sorted by SK_ID_CURR) \n# This allows us to actually see the results in a reasonable amount of time! \napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv').sort_values('SK_ID_CURR').reset_index(drop = True).loc[:1000, :]\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv').sort_values('SK_ID_CURR').reset_index(drop = True).loc[:1000, :]\nbureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv').sort_values(['SK_ID_CURR', 'SK_ID_BUREAU']).reset_index(drop = True).loc[:1000, :]\nbureau_balance = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv').sort_values('SK_ID_BUREAU').reset_index(drop = True).loc[:1000, :]\ncash = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]\ncredit = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]\nprevious = pd.read_csv('../input/home-credit-default-risk/previous_application.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]\ninstallments = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll join the train and test set together but add a separate column identifying the set. This is important because we are going to want to apply the same exact procedures to each dataset. It's safest to just join them together and treat them as a single dataframe.\n\n(I'm not sure if this is allowing data leakage into the train set and if these feature creation operations should be applied separately. Any thoughts would be much appreciated!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add identifying column\napp_train['set'] = 'train'\napp_test['set'] = 'test'\napp_test[\"TARGET\"] = np.nan\n\n# Append the dataframes\napp = app_train.append(app_test, ignore_index = True)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Entity set with id applications\nes = ft.EntitySet(id = 'clients')","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Entities and Entitysets\nAn entity is simply a table or in Pandas, a dataframe. The observations are in the rows and the features in the columns. An entity in featuretools must have a unique index where none of the elements are duplicated. Currently, only app, bureau, and previous have unique indices (SK_ID_CURR, SK_ID_BUREAU, and SK_ID_PREV respectively). For the other dataframes, we must pass in make_index = True and then specify the name of the index. Entities can also have time indices where each entry is identified by a unique time. (There are not datetimes in any of the data, but there are relative times, given in months or days, that we could consider treating as time variables).\n\nAn EntitySet is a collection of tables and the relationships between them. This can be thought of a data structute with its own methods and attributes. Using an EntitySet allows us to group together multiple tables and manipulate them much quicker than individual tables.\n\nFirst we'll make an empty entityset named clients to keep track of all the data."},{"metadata":{},"cell_type":"markdown","source":"Now we define each entity, or table of data. We need to pass in an index if the data has one or make_index = True if not. Featuretools will automatically infer the types of variables, but we can also change them if needed. For intstance, if we have a categorical variable that is represented as an integer we might want to let featuretools know the right type."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Entities with a unique index\nes = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR')\n\nes = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n\nes = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV')\n\n# Entities that do not have a unique index\nes = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n                              make_index = True, index = 'bureaubalance_index')\n\nes = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n                              make_index = True, index = 'cash_index')\n\nes = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n                              make_index = True, index = 'installments_index')\n\nes = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n                              make_index = True, index = 'credit_index')","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Relationships\nRelationships are a fundamental concept not only in featuretools, but in any relational database. The best way to think of a one-to-many relationship is with the analogy of parent-to-child. A parent is a single individual, but can have mutliple children. The children can then have multiple children of their own. In a parent table, each individual has a single row. Each individual in the parent table can have multiple rows in the child table.\n\nAs an example, the app dataframe has one row for each client (SK_ID_CURR) while the bureau dataframe has multiple previous loans (SK_ID_PREV) for each parent (SK_ID_CURR). Therefore, the bureau dataframe is the child of the app dataframe. The bureau dataframe in turn is the parent of bureau_balance because each loan has one row in bureau but multiple monthly records in bureau_balance."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Parent: app, Parent Variable: SK_ID_CURR\\n\\n', app.iloc[:, 111:115].head())\nprint('\\nChild: bureau, Child Variable: SK_ID_CURR\\n\\n', bureau.iloc[10:30, :4].head())","execution_count":13,"outputs":[{"output_type":"stream","text":"Parent: app, Parent Variable: SK_ID_CURR\n\n    FLAG_DOCUMENT_17  FLAG_DOCUMENT_18  FLAG_DOCUMENT_19  FLAG_DOCUMENT_20\n0                 0                 0                 0                 0\n1                 0                 0                 0                 0\n2                 0                 0                 0                 0\n3                 0                 0                 0                 0\n4                 0                 0                 0                 0\n\nChild: bureau, Child Variable: SK_ID_CURR\n\n     SK_ID_CURR  SK_ID_BUREAU CREDIT_ACTIVE CREDIT_CURRENCY\n10      100002       6158905        Closed      currency 1\n11      100002       6158906        Closed      currency 1\n12      100002       6158907        Closed      currency 1\n13      100002       6158908        Closed      currency 1\n14      100002       6158909        Active      currency 1\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The SK_ID_CURR \"100002\" has one row in the parent table and multiple rows in the child.\n\nTwo tables are linked via a shared variable. The app and bureau dataframe are linked by the SK_ID_CURR variable while the bureau and bureau_balance dataframes are linked with the SK_ID_BUREAU. Defining the relationships is relatively straightforward, and the diagram provided by the competition is helpful for seeing the relationships. For each relationship, we need to specify the parent variable and the child variable. Altogether, there are a total of 6 relationships between the tables. Below we specify all six relationships and then add them to the EntitySet."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relationship between app and bureau\nr_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n\n# Relationship between bureau and bureau balance\nr_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n\n# Relationship between current app and previous apps\nr_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n\n# Relationships between previous apps and cash, installments, and credit\nr_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\nr_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\nr_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add in the defined relationships\nes = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n                           r_previous_cash, r_previous_installments, r_previous_credit])\n# Print out the EntitySet\nes","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"Entityset: clients\n  Entities:\n    app [Rows: 2002, Columns: 123]\n    bureau [Rows: 1001, Columns: 17]\n    previous [Rows: 1001, Columns: 37]\n    bureau_balance [Rows: 1001, Columns: 4]\n    cash [Rows: 1001, Columns: 9]\n    installments [Rows: 1001, Columns: 9]\n    credit [Rows: 1001, Columns: 24]\n  Relationships:\n    bureau.SK_ID_CURR -> app.SK_ID_CURR\n    bureau_balance.SK_ID_BUREAU -> bureau.SK_ID_BUREAU\n    previous.SK_ID_CURR -> app.SK_ID_CURR\n    cash.SK_ID_PREV -> previous.SK_ID_PREV\n    installments.SK_ID_PREV -> previous.SK_ID_PREV\n    credit.SK_ID_PREV -> previous.SK_ID_PREV"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Slightly advanced note: we need to be careful to not create a diamond graph where there are multiple paths from a parent to a child. If we directly link app and cash via SK_ID_CURR; previous and cash via SK_ID_PREV; and app and previous via SK_ID_CURR, then we have created two paths from app to cash. This results in ambiguity, so the approach we have to take instead is to link app to cash through previous. We establish a relationship between previous (the parent) and cash (the child) using SK_ID_PREV. Then we establish a relationship between app (the parent) and previous (now the child) using SK_ID_CURR. Then featuretools will be able to create features on app derived from both previous and cash by stacking multiple primitives.\n\nAll entities in the entity can be related to each other. In theory this allows us to calculate features for any of the entities, but in practice, we will only calculate features for the app dataframe since that will be used for training/testing."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}